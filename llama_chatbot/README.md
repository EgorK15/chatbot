# Llama Chatbot

Чат-бот на базе модели Llama с веб-интерфейсом на Streamlit и интеграцией через LangChain.

## Возможности

- Чат с моделью Llama через OpenAI-совместимый API
- Поддержка нескольких чатов с возможностью переключения между ними
- Структурированный вывод ответов модели с оценкой уверенности и источниками
- Настройка параметров модели и API через веб-интерфейс

## Требования

- Python 3.11 или выше
- Pipenv для управления зависимостями
- Доступ к API Llama через OpenAI-совместимый интерфейс
- Docker и Docker Compose (опционально)

## Установка и запуск без Docker

1. Клонируйте репозиторий или создайте новую директорию для проекта:

```bash
mkdir llama_chatbot
cd llama_chatbot
```

2. Установите зависимости с помощью Pipenv:

```bash
pipenv install
```

Или установите зависимости вручную:

```bash
pipenv install streamlit langchain langchain-openai openai
```

3. Активируйте виртуальное окружение:

```bash
pipenv shell
```

4. Запустите приложение Streamlit:

```bash
streamlit run app.py
```

5. Откройте браузер по адресу http://localhost:8501

## Запуск с Docker

1. Соберите и запустите контейнер с помощью Docker Compose:

```bash
docker-compose up -d
```

2. Откройте браузер по адресу http://localhost:8501

Или используйте Docker напрямую:

```bash
docker build -t llama-chatbot .
docker run -p 8501:8501 llama-chatbot
```

## Настройка API через переменные окружения

Вы можете настроить API ключи через переменные окружения:

```bash
# Для Linux/macOS
export OPENAI_API_KEY="your-api-key"
export OPENAI_API_BASE="https://llama3gpu.neuraldeep.tech/v1"

# Для Windows (PowerShell)
$env:OPENAI_API_KEY="your-api-key"
$env:OPENAI_API_BASE="https://llama3gpu.neuraldeep.tech/v1"
```

## Особенности интерфейса

- **Чаты**: Создавайте, переименовывайте и удаляйте чаты в боковой панели
- **Настройки API**: Настраивайте параметры подключения к модели
- **Структурированный вывод**: Получайте структурированную информацию от модели

## Требования к API

Чат-бот предполагает, что вы имеете доступ к API, совместимому с OpenAI. Это может быть:

1. Локально запущенная модель Llama с API-сервером (например, через llama.cpp, llama-api или другое решение)
2. Внешний API-сервис, предоставляющий доступ к Llama

API должен быть совместим с форматом OpenAI для работы с langchain-openai.

## Возможные проблемы

- Если возникает ошибка при подключении к API, проверьте правильность URL и API ключа
- Убедитесь, что у вас установлены все необходимые зависимости
- При первом запуске в Docker может потребоваться некоторое время для загрузки зависимостей

## Дополнительные возможности

Вы можете расширить функционал чат-бота:

- Добавить сохранение истории чата
- Интегрировать другие модели
- Настроить дополнительные параметры запросов
- Добавить обработку различных типов содержимого (изображения, файлы и т.д.) 