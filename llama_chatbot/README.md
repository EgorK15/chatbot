# Llama Chatbot

Чат-бот на базе модели Llama с веб-интерфейсом на Streamlit и интеграцией через LangChain.

## Возможности

- Чат с моделью Llama через OpenAI-совместимый API
- Поддержка нескольких чатов с возможностью переключения между ними
- Архивирование сообщений (Кнопка **"Очистить текущий чат"** архивирует (скрывает) все существующие сообщения в выбранном чате, позволяя новым сообщениям сохраняться и использоваться в контексте. Кнопка **"Удалить чат"** архивирует все сообщения и сам чат, удаляя его из видимого списка, но оставляя данные в базе для последующего анализа.)
- Структурированный вывод ответов модели с оценкой уверенности и источниками
- Настройка параметров модели и API через веб-интерфейс (ввод ключей, URL, модели и температуры)

## Требования

- **Python 3.10**1 или выше
- **Pipenv** для управления зависимостями
- Доступ к API Llama через OpenAI-совместимый интерфейс
- **SQLite** (используется для зранения истории чатов и сообщений
- Docker и Docker Compose (опционально)

## Установка и запуск без Docker

1. Клонируйте репозиторий или создайте новую директорию для проекта:

```bash
mkdir llama_chatbot
cd llama_chatbot
```

2. Установите зависимости с помощью Pipenv:

```bash
pipenv install
```

Или установите зависимости вручную:

```bash
pipenv install streamlit langchain langchain-openai openai
```

3. Активируйте виртуальное окружение:

```bash
pipenv shell
```

4. Запустите приложение Streamlit:

```bash
streamlit run app.py
```

5. Откройте браузер по адресу http://localhost:8501

## Запуск с Docker

1. Соберите и запустите контейнер с помощью Docker Compose:

```bash
docker-compose up -d
```

2. Откройте браузер по адресу http://localhost:8501

Или используйте Docker напрямую:

```bash
docker build -t llama-chatbot .
docker run -p 8501:8501 llama-chatbot
```

## Настройка API через переменные окружения

Вы можете настроить API ключи через переменные окружения:

```bash
# Для Linux/macOS
export OPENAI_API_KEY="your-api-key"
export OPENAI_API_BASE="your-model-link"

# Для Windows (PowerShell)
$env:OPENAI_API_KEY="your-api-key"
$env:OPENAI_API_BASE="your-model-link"
```

## Особенности интерфейса

- **Чаты**: Создавайте, переименовывайте и выбирайте чаты в боковой панели. Все сессии и их история сохраняются в базе данных, что позволяет восстанавливать их даже после перезапуска приложения
- **Настройки API**: В боковой панели можно настроить параметры подключения к модели (API ключ, URL, название модели, температура)
- **Структурированный вывод**: Включение этой опции позволяет получать от модели ответы в виде структурированного JSON, который отображается отдельно в интерфейсе
- **Архивирование сообщений**: можно архивировать чаты и сообщения

## Требования к API

Чат-бот предполагает, что вы имеете доступ к API, совместимому с OpenAI. Это может быть:

1. Локально запущенная модель Llama с API-сервером (например, через llama.cpp, llama-api или другое решение)
2. Внешний API-сервис, предоставляющий доступ к Llama

API должен быть совместим с форматом OpenAI для работы с langchain-openai.

## Возможные проблемы

- Если возникает ошибка при подключении к API, проверьте правильность URL и API ключа
- Убедитесь, что у вас установлены все необходимые зависимости
- При первом запуске в Docker может потребоваться некоторое время для загрузки зависимостей

## Дополнительные возможности

Вы можете расширить функционал чат-бота:

- Добавить возможность восстановления архивированных чатов
- Интегрировать другие модели
- Настроить дополнительные параметры запросов
- Добавить обработку различных типов содержимого (изображения, файлы и т.д.)
- Добавить аналитику (например, статистику по токенам или времени отклика модели)