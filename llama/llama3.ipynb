{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai\n",
    "#!pip install langchain requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Укажите URL вашего сервера с моделью llama3-8b-instruct-8k\n",
    "api_base = input(\"Введите адрес модели: \")\n",
    "my_model = input(\"Введите название модели: \")\n",
    "API_KEY = input(\"Введите ключ: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Получаем текущий рабочий каталог\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Строим путь на три уровня вверх\n",
    "config_path = os.path.join(current_dir, '..', '..', 'llama.config')\n",
    "\n",
    "# Нормализуем путь (убираем лишние '..' и т.д.)\n",
    "config_path = os.path.normpath(config_path)\n",
    "\n",
    "# Читаем файл\n",
    "with open(config_path, 'r') as file:\n",
    "    api_base = file.readline().strip()\n",
    "    my_model = file.readline().strip()\n",
    "    API_KEY = file.readline().strip()\n",
    "\n",
    "# Выводим значения\n",
    "#print(f\"API Base: {api_base}\")\n",
    "#print(f\"My Model: {my_model}\")\n",
    "#print(f\"API Key: {API_KEY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import logging\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28936\\161643322.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  my_chat = ChatOpenAI(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28936\\161643322.py:18: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = my_chat(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Настройка ChatOpenAI\n",
    "#llm = ChatOpenAI(\n",
    "my_chat = ChatOpenAI(\n",
    "    base_url = api_base,\n",
    "    model = my_model,\n",
    "    openai_api_key=API_KEY,  # Укажите ваш API-ключ (если требуется)\n",
    "    temperature=0.7,                # Настройте параметры модели\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Пример запроса\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\")\n",
    "]\n",
    "\n",
    "# Отправка запроса\n",
    "response = my_chat(messages)\n",
    "\n",
    "# Вывод ответа\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Пример работы в Jupyter Notebook**\n",
    "\n",
    "```\n",
    "Чат с Llama 3 (Для выхода введите 'стоп')\n",
    "You: расскажи что-нибуть интересное\n",
    "Llama 3: Конечно! Вот интересный факт: в 1952 году американский физик и изобретатель Роберт Вуд провел эксперимент, который стал классическим примером оптической иллюзии. Он построил устройство, которое позволяло ему исчезнуть на глазах у зрителей. Вуд использовал зеркала и специальное освещение, чтобы создать впечатление, что он исчезает и появляется в другом месте. Этот эксперимент стал популярным в научных шоу и показал, как оптические иллюзии могут обман\n",
    "You: кто такой роберт вуд?\n",
    "Llama 3: Роберт Вуд (Robert W. Wood) — это имя нескольких известных личностей, но наиболее известен из них физик и изобретатель, живший в конце XIX — начале XX века. Однако, если вы говорите о более современной фигуре, возможно, вы имеете в виду компанию или персону с таким же именем.\n",
    "\n",
    "1. **Роберт Вуд (1868-1955)** — американский физик и изобретатель, известный своими экспериментами с оптикой и инфракрасным излучением. Он также написал несколько научно-популярных книг\n",
    "Чат завершен\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_28936\\4013588632.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "# Настройка ChatOpenAI для LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=api_base,\n",
    "    model=my_model,\n",
    "    openai_api_key=API_KEY,  # Укажите ваш API-ключ (если требуется)\n",
    "    temperature=0.7,          # Настройте параметры модели\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "# Создаем шаблон для чата\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"input\"),  # Используем MessagesPlaceholder\n",
    "])\n",
    "\n",
    "# Создаем цепочку (chain)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "def chat():\n",
    "    print(\"Чат с Llama 3 (Для выхода введите 'стоп')\")\n",
    "    while True:\n",
    "        my_input = input(\"Вы: \")\n",
    "        if my_input.lower() == \"стоп\":\n",
    "            print(\"Чат завершен\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Передаем input в chain.run() как словарь\n",
    "            response = chain.run(input=[HumanMessage(content=my_input)])\n",
    "            print(f\"You: {my_input}\")\n",
    "\n",
    "            print(f\"Llama 3: {response}\")\n",
    "        except Exception as e:\n",
    "            print(\"Произошла ошибка. Пожалуйста, попробуйте снова!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чат с Llama 3 (Для выхода введите 'стоп')\n",
      "Чат завершен\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключил Structured Outputs отсюда:\n",
    "\n",
    "https://docs.vllm.ai/en/latest/features/structured_outputs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=api_base,\n",
    "    api_key=API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=my_model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify this sentiment: The world is bautiful!\"}\n",
    "    ],\n",
    "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=my_model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify this sentiment: I see trees of green, red roses too. I see them bloom for me and you. And I think to myself: What a wonderful world!\"}\n",
    "    ],\n",
    "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=my_model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Classify this sentiment: They are ugly and terrifying!\"}\n",
    "    ],\n",
    "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
